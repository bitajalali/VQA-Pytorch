{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session6-visual-question-answering.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxnzfBYqnZwJ",
        "colab_type": "code",
        "outputId": "54a718e5-176a-4e61-dd90-5c3e1214e5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/howsam/basic_vqa.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'basic_vqa' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MU0CuoIBLXD",
        "colab_type": "code",
        "outputId": "9f75f74b-5015-45dc-a276-d09151ba53ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd basic_vqa/utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/basic_vqa/utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqp6VTn31JQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/bin/tcsh\n",
        "\n",
        "#########################################################\n",
        "\n",
        "# One may need to change directory for datasets like this.\n",
        "#set DATASETS_DIR = \"/run/media/hoosiki/WareHouse3/mtb/datasets/VQA\"\n",
        "\n",
        "!mkdir -p \"../datasets\"\n",
        "DATASETS_DIR = \"../datasets\"\n",
        "\n",
        "##########################################################\n",
        "\n",
        "ANNOTATIONS_DIR = DATASETS_DIR+\"/Annotations\"\n",
        "QUESTIONS_DIR = DATASETS_DIR+\"/Questions\"\n",
        "IMAGES_DIR = DATASETS_DIR+\"/Images\"\n",
        "\n",
        "##########################################################\n",
        "\n",
        "!mkdir -p {ANNOTATIONS_DIR}\n",
        "!mkdir -p {QUESTIONS_DIR}\n",
        "!mkdir -p {IMAGES_DIR}\n",
        "\n",
        "##########################################################\n",
        "\n",
        "# Download datasets from VQA official url: https://visualqa.org/download.html\n",
        "\n",
        "# VQA Annotations\n",
        "# !wget -O {ANNOTATIONS_DIR}/v2_Annotations_Train_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\"\n",
        "!wget -O {ANNOTATIONS_DIR}/v2_Annotations_Val_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\"\n",
        "\n",
        "# VQA Input Questions\n",
        "# !wget -O {QUESTIONS_DIR}/v2_Questions_Train_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\"\n",
        "!wget -O {QUESTIONS_DIR}/v2_Questions_Val_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\"\n",
        "# !wget -O {QUESTIONS_DIR}/v2_Questions_Test_mscoco.zip \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip\"\n",
        "\n",
        "# VQA Input Images (COCO)\n",
        "# !wget -O {IMAGES_DIR}/train2014.zip \"http://images.cocodataset.org/zips/train2014.zip\"\n",
        "!wget -O {IMAGES_DIR}/val2014.zip \"http://images.cocodataset.org/zips/val2014.zip\"\n",
        "# !wget -O {IMAGES_DIR}/test2015.zip \"http://images.cocodataset.org/zips/test2015.zip\"\n",
        "\n",
        "##########################################################\n",
        "\n",
        "# !unzip {ANNOTATIONS_DIR}/v2_Annotations_Train_mscoco.zip -d {ANNOTATIONS_DIR}\n",
        "!unzip {ANNOTATIONS_DIR}/v2_Annotations_Val_mscoco.zip -d {ANNOTATIONS_DIR}\n",
        "\n",
        "# !unzip {QUESTIONS_DIR}/v2_Questions_Train_mscoco.zip -d {QUESTIONS_DIR}\n",
        "!unzip {QUESTIONS_DIR}/v2_Questions_Val_mscoco.zip -d {QUESTIONS_DIR}\n",
        "# !unzip {QUESTIONS_DIR}/v2_Questions_Test_mscoco.zip -d {QUESTIONS_DIR}\n",
        "\n",
        "# !unzip {IMAGES_DIR}/train2014.zip -d {IMAGES_DIR}\n",
        "!unzip {IMAGES_DIR}/val2014.zip -d {IMAGES_DIR}\n",
        "# !unzip {IMAGES_DIR}/test2015.zip -d {IMAGES_DIR}\n",
        "\n",
        "##########################################################\n",
        "\n",
        "# Remove unnecessary zip files.\n",
        "\n",
        "# !rm {ANNOTATIONS_DIR}/v2_Annotations_Train_mscoco.zip\n",
        "!rm {ANNOTATIONS_DIR}/v2_Annotations_Val_mscoco.zip\n",
        "\n",
        "# !rm {QUESTIONS_DIR}/v2_Questions_Train_mscoco.zip\n",
        "!rm {QUESTIONS_DIR}/v2_Questions_Val_mscoco.zip\n",
        "# !rm {QUESTIONS_DIR}/v2_Questions_Test_mscoco.zip\n",
        "\n",
        "# !rm {IMAGES_DIR}/train2014.zip\n",
        "!rm {IMAGES_DIR}/val2014.zip\n",
        "# !rm {IMAGES_DIR}/test2015.zip\n",
        "\n",
        "##########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6D2iW03xwrT",
        "colab_type": "code",
        "outputId": "4fb29ba9-4947-4995-e976-da9c9f00f106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!python resize_images.py --input_dir='../datasets/Images' --output_dir='../datasets/Resized_Images'  \n",
        "!python make_vacabs_for_questions_answers.py --input_dir='../datasets'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[2000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[3000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[4000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[5000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[6000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[7000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[8000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[9000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[10000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[11000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[12000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[13000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[14000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[15000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[16000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[17000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[18000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[19000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[20000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[21000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[22000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[23000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[24000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[25000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[26000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[27000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[28000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[29000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[30000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[31000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[32000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[33000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[34000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[35000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[36000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[37000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[38000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[39000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "[40000/40504] Resized the images and saved into '../datasets/Resized_Images/val2014'.\n",
            "Make vocabulary for questions\n",
            "The number of total words of questions: 10252\n",
            "Maximum length of question: 26\n",
            "Make vocabulary for answers\n",
            "The number of total words of answers: 78991\n",
            "Keep top 1000 answers into vocab\n",
            "Traceback (most recent call last):\n",
            "  File \"build_vqa_inputs.py\", line 5, in <module>\n",
            "    import text_processing\n",
            "ModuleNotFoundError: No module named 'text_processing'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI0gbz_N5F9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import argparse\n",
        "import text_helper as text_processing\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def extract_answers(q_answers, valid_answer_set):\n",
        "    all_answers = [answer[\"answer\"] for answer in q_answers]\n",
        "    valid_answers = [a for a in all_answers if a in valid_answer_set]\n",
        "    return all_answers, valid_answers\n",
        "\n",
        "\n",
        "def vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, image_set):\n",
        "    print('building vqa %s dataset' % image_set)\n",
        "    if image_set in ['val2014']:\n",
        "        load_answer = True\n",
        "        with open(annotation_file % image_set) as f:\n",
        "            annotations = json.load(f)['annotations']\n",
        "            qid2ann_dict = {ann['question_id']: ann for ann in annotations}\n",
        "    else:\n",
        "        load_answer = False\n",
        "    with open(question_file % image_set) as f:\n",
        "        questions = json.load(f)['questions']\n",
        "    coco_set_name = image_set.replace('-dev', '')\n",
        "    abs_image_dir = os.path.abspath(image_dir % coco_set_name)\n",
        "    image_name_template = 'COCO_'+coco_set_name+'_%012d'\n",
        "    dataset = [None]*len(questions)\n",
        "    \n",
        "    unk_ans_count = 0\n",
        "    for n_q, q in enumerate(questions):\n",
        "        if (n_q+1) % 10000 == 0:\n",
        "            print('processing %d / %d' % (n_q+1, len(questions)))\n",
        "        image_id = q['image_id']\n",
        "        question_id = q['question_id']\n",
        "        image_name = image_name_template % image_id\n",
        "        image_path = os.path.join(abs_image_dir, image_name+'.jpg')\n",
        "        question_str = q['question']\n",
        "        question_tokens = text_processing.tokenize(question_str)\n",
        "        \n",
        "        iminfo = dict(image_name=image_name,\n",
        "                      image_path=image_path,\n",
        "                      question_id=question_id,\n",
        "                      question_str=question_str,\n",
        "                      question_tokens=question_tokens)\n",
        "        \n",
        "        if load_answer:\n",
        "            ann = qid2ann_dict[question_id]\n",
        "            all_answers, valid_answers = extract_answers(ann['answers'], valid_answer_set)\n",
        "            if len(valid_answers) == 0:\n",
        "                valid_answers = ['<unk>']\n",
        "                unk_ans_count += 1\n",
        "            iminfo['all_answers'] = all_answers\n",
        "            iminfo['valid_answers'] = valid_answers\n",
        "            \n",
        "        dataset[n_q] = iminfo\n",
        "    print('total %d out of %d answers are <unk>' % (unk_ans_count, len(questions)))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "input_dir = '../datasets'\n",
        "output_dir = '../datasets'\n",
        "\n",
        "image_dir = input_dir+'/Resized_Images/%s/'\n",
        "annotation_file = input_dir+'/Annotations/v2_mscoco_%s_annotations.json'\n",
        "question_file = input_dir+'/Questions/v2_OpenEnded_mscoco_%s_questions.json'\n",
        "\n",
        "vocab_answer_file = output_dir+'/vocab_answers.txt'\n",
        "answer_dict = text_processing.VocabDict(vocab_answer_file)\n",
        "valid_answer_set = set(answer_dict.word_list)    \n",
        "\n",
        "# train = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'train2014')\n",
        "valid = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'val2014')\n",
        "# test = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'test2015')\n",
        "# test_dev = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'test-dev2015')\n",
        "\n",
        "np.save(output_dir+'/train.npy', np.array(valid))\n",
        "np.save(output_dir+'/valid.npy', np.array(valid))\n",
        "# np.save(output_dir+'/train_valid.npy', np.array(train+valid))\n",
        "# np.save(output_dir+'/test.npy', np.array(test))\n",
        "# np.save(output_dir+'/test-dev.npy', np.array(test_dev))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11kNvohRO9YN",
        "colab_type": "code",
        "outputId": "86bd2db4-8f81-47bf-a3b3-ba1003dcb3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/basic_vqa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikYjaSllPjC9",
        "colab_type": "code",
        "outputId": "ebb37841-a99d-4918-ceae-dbaf958e2f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 178, in <module>\n",
            "    main(args)\n",
            "  File \"train.py\", line 27, in main\n",
            "    num_workers=args.num_workers)\n",
            "  File \"/content/basic_vqa/data_loader.py\", line 71, in get_loader\n",
            "    transform=transform['train']),\n",
            "  File \"/content/basic_vqa/data_loader.py\", line 14, in __init__\n",
            "    self.vqa = np.load(input_dir+'/'+input_vqa)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 447, in load\n",
            "    pickle_kwargs=pickle_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\", line 696, in read_array\n",
            "    raise ValueError(\"Object arrays cannot be loaded when \"\n",
            "ValueError: Object arrays cannot be loaded when allow_pickle=False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeHOZfAGRQXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class ImgEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"(1) Load the pretrained model as you want.\n",
        "               cf) one needs to check structure of model using 'print(model)'\n",
        "                   to remove last fc layer from the model.\n",
        "           (2) Replace final fc layer (score values from the ImageNet)\n",
        "               with new fc layer (image feature).\n",
        "           (3) Normalize feature vector.\n",
        "        \"\"\"\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        in_features = model.classifier[-1].in_features  # input size of feature vector\n",
        "        model.classifier = nn.Sequential(\n",
        "            *list(model.classifier.children())[:-1])    # remove last fc layer\n",
        "\n",
        "        self.model = model                              # loaded model without last fc layer\n",
        "        self.fc = nn.Linear(in_features, embed_size)    # feature vector of image\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"Extract feature vector from image vector.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.model(image)                  # [batch_size, vgg16(19)_fc=4096]\n",
        "        img_feature = self.fc(img_feature)                   # [batch_size, embed_size]\n",
        "        # img_feature = torch.Normalize(img_feature)\n",
        "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
        "        img_feature = img_feature.div(l2_norm)               # l2-normalized feature vector\n",
        "\n",
        "        return img_feature\n",
        "\n",
        "\n",
        "class QstEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(QstEncoder, self).__init__()\n",
        "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n",
        "\n",
        "    def forward(self, question):\n",
        "\n",
        "        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n",
        "        qst_vec = self.tanh(qst_vec)\n",
        "        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n",
        "        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n",
        "        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n",
        "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n",
        "        qst_feature = self.tanh(qst_feature)\n",
        "        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n",
        "\n",
        "        return qst_feature\n",
        "\n",
        "\n",
        "class VqaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(VqaModel, self).__init__()\n",
        "        self.img_encoder = ImgEncoder(embed_size)\n",
        "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(embed_size, ans_vocab_size)\n",
        "        self.fc2 = nn.Linear(ans_vocab_size, ans_vocab_size)\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "\n",
        "        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n",
        "        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  # [batch_size, embed_size]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)           # [batch_size, ans_vocab_size=1000]\n",
        "\n",
        "        return combined_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct6Cg9eyXLzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class imgencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(imgencoder, self).__init__()\n",
        "        self.model = models.resnet18(True)\n",
        "        self.model.fc = nn.Linear(512, 1024)\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_feature = self.model(img)\n",
        "        return img_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTc_K13bYFrb",
        "colab_type": "code",
        "outputId": "db452588-a829-493b-da92-24a5517d837d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "img_rand = torch.rand(10, 3, 224, 244).to(device)\n",
        "img_model = imgencoder().to(device)\n",
        "feature_rand = img_model(img_rand)\n",
        "\n",
        "print(feature_rand.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt9u7apKZK0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class txtencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(txtencoder, self).__init__()\n",
        "        self.word2vec = nn.Embedding(1000, 300)\n",
        "        self.lstm = nn.LSTM(300, 512, 2, batch_first=True)\n",
        "        self.fc = nn.Linear(2048, 1024)\n",
        "\n",
        "    def forward(self, txt):\n",
        "        embedd = self.word2vec(txt)\n",
        "        _, (hidden, cell) = lstm(embedd)\n",
        "        vec = torch.cat(hidden, cell)\n",
        "        vec = vec.view(txt.size(0), -1)\n",
        "        txt_feature = self.fc(vec)\n",
        "        return txt_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7C6Qu8_WnRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class combine(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(combine, self).__init__()\n",
        "        self.img_model = imgencoder()\n",
        "        self.txt_model = txtencoder()\n",
        "        self.fc1 = nn.Linear(1024, 1000)\n",
        "        self.fc2 = nn.Linear(1024, 1000)\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "        img_feature = self.img_model(img)\n",
        "        qst_feature = self.txt_model(qst)\n",
        "        combined_feature = img_feature * qst_feature\n",
        "        combined_feature = self.fc1(combined_feature)\n",
        "        combined_feature = torch.nn.functional.relu(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)\n",
        "        return combined_feature"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}